import os
from typing import List
import streamlit as st
from audio_recorder_streamlit import audio_recorder
from PIL import Image
from LLM import get_llm_backend, build_vectorstore_from_pdfs, message
from LLM import (
    APP_TITLE,
    init_session_state,
    transcribe_audio_bytes,
    ask_llm,
    speak_text,
    autoplay_audio_from_file,
    get_selected_images,
)

# ì„¸ì…˜ ì¤€ë¹„
init_session_state()

# ===== UI ì‹œì‘ =====
st.title(APP_TITLE)

# 1ï¸âƒ£ ìŒì„± ì¸ì‹ (ìë™ ì§ˆë¬¸/ì‘ë‹µ/TTS)
with st.container():
    st.markdown("### 1ï¸âƒ£ ìŒì„± ì¸ì‹ (ìë™ ì§ˆë¬¸/ì‘ë‹µ)")
    st.caption("ë§í•˜ê³  ë©ˆì¶”ë©´ ìë™ìœ¼ë¡œ ë¬¸ì¥ ë³€í™˜ â†’ ë‹µë³€ ìƒì„± â†’ ì„ íƒ ì‹œ ìŒì„±ìœ¼ë¡œ ì¬ìƒí•©ë‹ˆë‹¤.")
    audio_bytes = audio_recorder(text="ë§í•˜ê¸°")
    tts_auto = st.checkbox("ìŒì„± ë‹µë³€ ìë™ ì¬ìƒ", value=True)

    if audio_bytes:
        text_in = transcribe_audio_bytes(audio_bytes)
        if text_in:
            st.success(f"ì¸ì‹ëœ ì§ˆë¬¸: {text_in}")
            images = get_selected_images()
            with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                answer = ask_llm(text_in, images=images)
            st.markdown("#### ë‹µë³€")
            st.markdown(answer)
            if tts_auto:
                fn = speak_text(answer)
                if fn and os.path.exists(fn):
                    autoplay_audio_from_file(fn)

# 2ï¸âƒ£ ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì—¬ëŸ¬ ì¥)
with st.container():
    st.markdown("### 2ï¸âƒ£ ì´ë¯¸ì§€ ì—…ë¡œë“œ")
    files = st.file_uploader(
        "ê³µì • ì‚¬ì§„/ë„í‘œ ì—¬ëŸ¬ ì¥ ì—…ë¡œë“œ",
        type=["png", "jpg", "jpeg"],
        accept_multiple_files=True
    )
    if files:
        imgs: List[Image.Image] = []
        for f in files:
            try:
                img = Image.open(f).convert("RGB")
                imgs.append(img)
            except Exception:
                continue
        if imgs:
            st.session_state.upload_images = imgs
            st.image(imgs, caption=[f"ì—…ë¡œë“œ {i+1}" for i in range(len(imgs))])

    col_u1, col_u2 = st.columns(2)
    with col_u1:
        st.session_state.use_upload_for_next = st.checkbox("ì´ë²ˆ ì§ˆë¬¸ì— ì—…ë¡œë“œ ì´ë¯¸ì§€ í¬í•¨")
    with col_u2:
        if st.button("ì—…ë¡œë“œ ì´ë¯¸ì§€ ë¹„ìš°ê¸°"):
            st.session_state.upload_images = []
            st.success("ì—…ë¡œë“œ ì´ë¯¸ì§€ ëª©ë¡ì„ ë¹„ì› ìŠµë‹ˆë‹¤.")

# 3ï¸âƒ£ ì¹´ë©”ë¼ ì´¬ì˜ (ì—¬ëŸ¬ ì¥)
with st.container():
    st.markdown("### 3ï¸âƒ£ ì¹´ë©”ë¼ ì´¬ì˜")
    cam = st.camera_input("ì¹´ë©”ë¼ë¡œ ì´¬ì˜")
    if cam is not None:
        try:
            imgc = Image.open(cam).convert("RGB")
            st.session_state.camera_images.append(imgc)
            st.success("ì¹´ë©”ë¼ ì´ë¯¸ì§€ê°€ ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception:
            st.warning("ì¹´ë©”ë¼ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    if st.session_state.camera_images:
        st.image(
            st.session_state.camera_images,
            caption=[f"ì¹´ë©”ë¼ {i+1}" for i in range(len(st.session_state.camera_images))]
        )

    col_c1, col_c2 = st.columns(2)
    with col_c1:
        st.session_state.use_camera_for_next = st.checkbox("ì´ë²ˆ ì§ˆë¬¸ì— ì¹´ë©”ë¼ ì´ë¯¸ì§€ í¬í•¨")
    with col_c2:
        if st.button("ì¹´ë©”ë¼ ì´ë¯¸ì§€ ë¹„ìš°ê¸°"):
            st.session_state.camera_images = []
            st.success("ì¹´ë©”ë¼ ì´ë¯¸ì§€ ëª©ë¡ì„ ë¹„ì› ìŠµë‹ˆë‹¤.")
    st.caption("ë‘ ì²´í¬ ëª¨ë‘ ì¼œë©´ ì¹´ë©”ë¼ ì´ë¯¸ì§€ê°€ ë¨¼ì €, ì—…ë¡œë“œ ì´ë¯¸ì§€ê°€ ë‹¤ìŒìœ¼ë¡œ í•¨ê»˜ ì „ì†¡ë©ë‹ˆë‹¤.")

# ì±—ë´‡
st.markdown("---")
st.header("ì§ˆì˜ì‘ë‹µ")

if "chat_dialog" not in st.session_state:
    st.session_state.chat_dialog = []   # [{'role':'user'|'assistant','content':str}]

# ìƒë‹¨ íˆ´ë°”: ëŒ€í™” ì´ˆê¸°í™” + ìŒì„± ì¬ìƒ ì²´í¬ë°•ìŠ¤
col_a, col_b, col_c = st.columns([1, 1, 8])
with col_a:
    if st.button("ëŒ€í™” ì´ˆê¸°í™”", key="btn_clear_chat", use_container_width=True):
        st.session_state.chat_dialog = []
        # ask_llm ë‚´ë¶€ historyë¥¼ ê°™ì´ ì“°ì‹ ë‹¤ë©´ ì•„ë˜ë„ í•¨ê»˜ ë¹„ìš°ì„¸ìš” (ì„ íƒ)
        if "history" in st.session_state:
            st.session_state.history = []
        st.toast("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ§¹")
        (st.rerun if hasattr(st, "rerun") else st.experimental_rerun)()
with col_b:
    tts_chat = st.checkbox("ì±—ë´‡ ìŒì„± ì‘ë‹µ", value=True)

# ê³¼ê±° ëŒ€í™” ë Œë” (ìµœê·¼ì´ ì•„ë˜ë¡œ ë‚´ë ¤ì˜¤ë„ë¡ ìˆœì„œëŒ€ë¡œ ì¶œë ¥)
if st.session_state.chat_dialog:
    for msg in st.session_state.chat_dialog:
        with st.chat_message("user" if msg["role"] == "user" else "assistant"):
            st.markdown(msg["content"])

# ì…ë ¥ í¼(í…ìŠ¤íŠ¸ + ì„ íƒì  ìŒì„± ì…ë ¥)
with st.form("chat_form", clear_on_submit=True):
    user_text = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦ (ì˜ˆ: EUVì™€ DUV ì°¨ì´)", key="chat_text")
    use_mic   = st.checkbox("ìŒì„± ì…ë ¥ ì‚¬ìš©", value=False)
    audio_chat_bytes = audio_recorder(text="ë…¹ìŒ", key="chat_mic") if use_mic else None
    submitted = st.form_submit_button("Send")

# ì œì¶œ ì²˜ë¦¬
if submitted:
    # 1) ìŒì„± ì…ë ¥ì´ ìˆê³  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŒì„±ë¶€í„° ë¬¸ìë¡œ ë³€í™˜
    if (not user_text or not user_text.strip()) and audio_chat_bytes:
        try:
            user_text = transcribe_audio_bytes(audio_chat_bytes) or ""
        except Exception:
            user_text = ""

    if not user_text or not user_text.strip():
        st.info("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ ìŒì„±ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”.")
    else:
        # 2) ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡ ë° í‘œì‹œ
        st.session_state.chat_dialog.append({"role": "user", "content": user_text})
        with st.chat_message("user"):
            st.markdown(user_text)

        # 3) ìµœê·¼ ëŒ€í™” ë§¥ë½(ê°„ë‹¨) êµ¬ì„±
        #   - ë§ˆì§€ë§‰ 6ê°œ ë©”ì‹œì§€ë¥¼ 'ì‚¬ìš©ì/ë„ìš°ë¯¸:' í˜•íƒœë¡œ í•©ì³ LLMì— ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
        tail = st.session_state.chat_dialog[-12:]  # user/assistant í•©ì³ ìµœëŒ€ 12ê°œ = ìµœê·¼ 6í„´
        hist_lines = []
        for m in tail:
            role = "ì‚¬ìš©ì" if m["role"] == "user" else "ë„ìš°ë¯¸"
            hist_lines.append(f"{role}: {m['content']}")
        hist_txt = "\n".join(hist_lines)

        # 4) ì´ë¯¸ì§€ ì„ íƒ(ì—…ë¡œë“œ/ì¹´ë©”ë¼) í¬í•¨
        images = get_selected_images()

        # 5) í”„ë¡¬í”„íŠ¸ êµ¬ì„± â†’ ask_llm í˜¸ì¶œ
        #    (LLM.pyì˜ SYSTEM_PROMPTê°€ ì ìš©ë˜ë©°, ì—¬ê¸°ì„œëŠ” ëŒ€í™” ë§¥ë½ì„ í…ìŠ¤íŠ¸ë¡œ ì£¼ì…)
        full_prompt = (
            "ì•„ë˜ì˜ ìµœê·¼ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬, ì´ì–´ì§€ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •ì¤‘í•œ í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\n"
            f"[ìµœê·¼ ëŒ€í™”]\n{hist_txt or '(ì´ì „ ëŒ€í™” ì—†ìŒ)'}\n\n"
            f"[ì§ˆë¬¸]\n{user_text}"
        )

        with st.chat_message("assistant"):
            with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                answer = ask_llm(full_prompt, images=images)

            st.markdown(answer)
            # 6) ì‘ë‹µ ì €ì¥
            st.session_state.chat_dialog.append({"role": "assistant", "content": answer})

            # 7) ìŒì„± ì‘ë‹µ(ì„ íƒ)
            if tts_chat:
                fn = speak_text(answer)
                if fn and os.path.exists(fn):
                    autoplay_audio_from_file(fn)

